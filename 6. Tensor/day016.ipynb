{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분석 (koNLPy)\n",
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('그렇다', 'Adjective'), ('ㅋㅋㅋ', 'KoreanParticle'), ('.', 'Punctuation')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\djagk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\konlpy\\tag\\_okt.py:17: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "## 사용\n",
    "from konlpy.tag import Twitter\n",
    "twitter=Twitter()\n",
    "malist=twitter.pos('그래욬ㅋㅋㅋ.',norm=True,stem=True)\n",
    "print(malist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\djagk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\konlpy\\tag\\_okt.py:17: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "것(1030)그(896)말(833)안(445)용이(344)눈(323)평산(320)사람(315)내(303)놈(295)이(289)길(278)소리(272)일(268)거(257)얼굴(255)생각(242)못(241)수(234)치수(228)집(222)나(222)댁(221)때(213)네(206)강청댁(198)머(193)봉(187)서방(185)년(183)최(174)제(173)더(172)어디(164)강(159)마을(149)니(148)날(140)뒤(138)기(136)칠성(136)포수(136)녀(135)김(135)임(134)양반(132)아이(131)서희(131)하나(123)속(122)\n"
     ]
    }
   ],
   "source": [
    "#### Twitter를 이용한 명사 카운팅\n",
    "from konlpy.tag import Twitter,Okt\n",
    "fp=open('data/토지1.txt',encoding='utf8')\n",
    "text=fp.read()\n",
    "# 텍스트 한줄 처리\n",
    "twitter=Twitter()\n",
    "word_dic={}\n",
    "lines=text.split('\\n')\n",
    "for line in lines:\n",
    "    malist=twitter.pos(line)\n",
    "    for word in malist:\n",
    "        if word[1]=='Noun':\n",
    "            if not (word[0] in word_dic):\n",
    "                word_dic[word[0]]=0\n",
    "            word_dic[word[0]]+=1    # 카운트 처리\n",
    "\n",
    "# 정렬 :  가장 많이 사용된 명사부터 출력\n",
    "keys=sorted(word_dic.items(),key=lambda x:x[1],reverse=True)    # key-> 이것 기준으로 정렬 reverse=True 내림차순\n",
    "for word, count in keys[:50]:\n",
    "    print('{0}({1})'.format(word,count),end='')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Word2Vec으로 문장을 벡터로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "## Gensim의 Word2Vec으로 '토지'를 읽어오기\n",
    "from konlpy.tag import Twitter,Okt\n",
    "from gensim.models import word2vec\n",
    "fp=open('data/토지1.txt',encoding='utf8')\n",
    "text=fp.read()\n",
    "# 텍스트 한줄씩 처리\n",
    "twitter=Twitter()\n",
    "results=[]\n",
    "lines=text.split('\\n')\n",
    "for line in lines:\n",
    "    # 형태소 분석\n",
    "    # 단어의 기본형 사용\n",
    "    malist=twitter.pos(line,norm=True,stem=True)\n",
    "    r=[]\n",
    "    for word in malist:\n",
    "        # 어미/조사/구두점 등은 대상에서 제외\n",
    "        if not word[1] in ['Josa','Eomi','Punctuation']:\n",
    "            r.append(word[0])\n",
    "    r1=(' '.join(r)).strip()\n",
    "    results.append(r1)\n",
    "    #print(r1)\n",
    "\n",
    "# 파일로 출력\n",
    "wakati_file='toji.wakati'\n",
    "with open(\"data/\"+wakati_file,'w',encoding='utf8') as fp:\n",
    "    fp.write('\\n'.join(results))\n",
    "\n",
    "# Word2Vec 모델 만들기\n",
    "data=word2vec.LineSentence(\"data/\"+wakati_file)\n",
    "model=word2vec.Word2Vec(data,vector_size=200,window=10,hs=1,min_count=2,sg=1)\n",
    "model.save('data/toji.model')\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "model=word2vec.Word2Vec.load('data/toji.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('제', 0.7384454607963562),\n",
       " ('구석', 0.7312481999397278),\n",
       " ('양식', 0.7289553284645081),\n",
       " ('노류', 0.7168835997581482),\n",
       " ('남정', 0.7057181596755981),\n",
       " ('참견', 0.7000254988670349),\n",
       " ('생깄', 0.6997794508934021),\n",
       " ('집아', 0.6951684355735779),\n",
       " ('소나', 0.6819665431976318),\n",
       " ('그까짓', 0.6799244284629822)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['집'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\djagk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\konlpy\\tag\\_okt.py:17: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current-0\n",
      "current-20000\n",
      "current-40000\n",
      "current-60000\n",
      "current-80000\n",
      "current-100000\n",
      "current-120000\n",
      "current-140000\n",
      "current-160000\n",
      "current-180000\n",
      "current-200000\n",
      "current-220000\n",
      "current-240000\n",
      "current-260000\n",
      "current-280000\n",
      "current-300000\n",
      "current-320000\n",
      "current-340000\n",
      "current-360000\n",
      "current-380000\n",
      "current-400000\n",
      "current-420000\n",
      "current-440000\n",
      "current-460000\n",
      "current-480000\n",
      "current-500000\n",
      "current-520000\n",
      "current-540000\n",
      "current-560000\n",
      "current-580000\n",
      "current-600000\n",
      "current-620000\n",
      "current-640000\n",
      "current-660000\n",
      "current-680000\n",
      "current-700000\n",
      "current-720000\n",
      "current-740000\n",
      "current-760000\n",
      "current-780000\n",
      "current-800000\n",
      "current-820000\n",
      "current-840000\n",
      "current-860000\n",
      "current-880000\n",
      "current-900000\n",
      "current-920000\n",
      "current-940000\n",
      "current-960000\n",
      "current-980000\n",
      "current-1000000\n",
      "current-1020000\n",
      "current-1040000\n",
      "current-1060000\n",
      "current-1080000\n",
      "current-1100000\n",
      "current-1120000\n",
      "current-1140000\n",
      "current-1160000\n",
      "current-1180000\n",
      "current-1200000\n",
      "current-1220000\n",
      "current-1240000\n",
      "current-1260000\n",
      "current-1280000\n",
      "current-1300000\n",
      "current-1320000\n",
      "current-1340000\n",
      "current-1360000\n",
      "current-1380000\n",
      "current-1400000\n",
      "current-1420000\n",
      "current-1440000\n",
      "current-1460000\n",
      "current-1480000\n",
      "current-1500000\n",
      "current-1520000\n",
      "current-1540000\n",
      "current-1560000\n",
      "current-1580000\n",
      "current-1600000\n",
      "current-1620000\n",
      "current-1640000\n",
      "current-1660000\n",
      "current-1680000\n",
      "current-1700000\n",
      "current-1720000\n",
      "current-1740000\n",
      "current-1760000\n",
      "current-1780000\n",
      "current-1800000\n",
      "current-1820000\n",
      "current-1840000\n",
      "current-1860000\n",
      "current-1880000\n",
      "current-1900000\n",
      "current-1920000\n",
      "current-1940000\n",
      "current-1960000\n",
      "current-1980000\n",
      "current-2000000\n",
      "current-2020000\n",
      "current-2040000\n",
      "current-2060000\n",
      "current-2080000\n",
      "current-2100000\n",
      "current-2120000\n",
      "current-2140000\n",
      "current-2160000\n",
      "current-2180000\n",
      "current-2200000\n",
      "current-2220000\n",
      "current-2240000\n",
      "current-2260000\n",
      "current-2280000\n",
      "current-2300000\n",
      "current-2320000\n",
      "current-2340000\n",
      "current-2360000\n",
      "current-2380000\n",
      "current-2400000\n",
      "current-2420000\n",
      "current-2440000\n",
      "current-2460000\n",
      "current-2480000\n",
      "current-2500000\n",
      "current-2520000\n",
      "current-2540000\n",
      "current-2560000\n",
      "current-2580000\n",
      "current-2600000\n",
      "current-2620000\n",
      "current-2640000\n",
      "current-2660000\n",
      "current-2680000\n",
      "current-2700000\n",
      "current-2720000\n",
      "current-2740000\n",
      "current-2760000\n",
      "current-2780000\n",
      "current-2800000\n",
      "current-2820000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\python\\Tensor\\day016.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/python/Tensor/day016.ipynb#ch0000008?line=19'>20</a>\u001b[0m i\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/python/Tensor/day016.ipynb#ch0000008?line=20'>21</a>\u001b[0m \u001b[39m# 형태소 분석\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/python/Tensor/day016.ipynb#ch0000008?line=21'>22</a>\u001b[0m malist\u001b[39m=\u001b[39mtwitter\u001b[39m.\u001b[39;49mpos(line,norm\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,stem\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/python/Tensor/day016.ipynb#ch0000008?line=22'>23</a>\u001b[0m \u001b[39m# 필요한 어구만 대상으로 하기\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/python/Tensor/day016.ipynb#ch0000008?line=23'>24</a>\u001b[0m r\u001b[39m=\u001b[39m[]\n",
      "File \u001b[1;32mc:\\Users\\djagk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\konlpy\\tag\\_okt.py:71\u001b[0m, in \u001b[0;36mOkt.pos\u001b[1;34m(self, phrase, norm, stem, join)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/djagk/AppData/Local/Programs/Python/Python310/lib/site-packages/konlpy/tag/_okt.py?line=58'>59</a>\u001b[0m \u001b[39m\"\"\"POS tagger.\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/djagk/AppData/Local/Programs/Python/Python310/lib/site-packages/konlpy/tag/_okt.py?line=59'>60</a>\u001b[0m \u001b[39mIn contrast to other classes in this subpackage,\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/djagk/AppData/Local/Programs/Python/Python310/lib/site-packages/konlpy/tag/_okt.py?line=60'>61</a>\u001b[0m \u001b[39mthis POS tagger doesn't have a `flatten` option,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/djagk/AppData/Local/Programs/Python/Python310/lib/site-packages/konlpy/tag/_okt.py?line=66'>67</a>\u001b[0m \u001b[39m:param join: If True, returns joined sets of morph and tag.\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/djagk/AppData/Local/Programs/Python/Python310/lib/site-packages/konlpy/tag/_okt.py?line=67'>68</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/djagk/AppData/Local/Programs/Python/Python310/lib/site-packages/konlpy/tag/_okt.py?line=68'>69</a>\u001b[0m validate_phrase_inputs(phrase)\n\u001b[1;32m---> <a href='file:///c%3A/Users/djagk/AppData/Local/Programs/Python/Python310/lib/site-packages/konlpy/tag/_okt.py?line=70'>71</a>\u001b[0m tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjki\u001b[39m.\u001b[39;49mtokenize(\n\u001b[0;32m     <a href='file:///c%3A/Users/djagk/AppData/Local/Programs/Python/Python310/lib/site-packages/konlpy/tag/_okt.py?line=71'>72</a>\u001b[0m             phrase,\n\u001b[0;32m     <a href='file:///c%3A/Users/djagk/AppData/Local/Programs/Python/Python310/lib/site-packages/konlpy/tag/_okt.py?line=72'>73</a>\u001b[0m             jpype\u001b[39m.\u001b[39;49mjava\u001b[39m.\u001b[39;49mlang\u001b[39m.\u001b[39;49mBoolean(norm),\n\u001b[0;32m     <a href='file:///c%3A/Users/djagk/AppData/Local/Programs/Python/Python310/lib/site-packages/konlpy/tag/_okt.py?line=73'>74</a>\u001b[0m             jpype\u001b[39m.\u001b[39;49mjava\u001b[39m.\u001b[39;49mlang\u001b[39m.\u001b[39;49mBoolean(stem))\u001b[39m.\u001b[39mtoArray()\n\u001b[0;32m     <a href='file:///c%3A/Users/djagk/AppData/Local/Programs/Python/Python310/lib/site-packages/konlpy/tag/_okt.py?line=74'>75</a>\u001b[0m \u001b[39mif\u001b[39;00m join:\n\u001b[0;32m     <a href='file:///c%3A/Users/djagk/AppData/Local/Programs/Python/Python310/lib/site-packages/konlpy/tag/_okt.py?line=75'>76</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [t \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m tokens]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## wiki.txt\n",
    "from konlpy.tag import Twitter, Okt\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# 파일 열기\n",
    "read_file=open('data/wiki.txt','r',encoding='utf8')\n",
    "wakati_file='wiki.wakati'\n",
    "writeFp=open('data/'+wakati_file,'w',encoding='utf8')\n",
    "\n",
    "# 형태소 분석\n",
    "twitter=Twitter()\n",
    "i=0\n",
    "# 텍스트 한줄씩 처리\n",
    "while True:\n",
    "    line=read_file.readline()\n",
    "    if not line:\n",
    "        break\n",
    "    if i%20000==0:\n",
    "        print('current-'+str(i))\n",
    "    i+=1\n",
    "    # 형태소 분석\n",
    "    malist=twitter.pos(line,norm=True,stem=True)\n",
    "    # 필요한 어구만 대상으로 하기\n",
    "    r=[]\n",
    "    for word in malist:\n",
    "        # 어미/조사/구두점 등 제외\n",
    "        if not word[1] in ['Josa','Eomi','Punctuation']:\n",
    "            writeFp.write(word[0]+' ')\n",
    "writeFp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "data=word2vec.Text8Corpus('data/wiki.wakati')\n",
    "model=word2vec.Word2Vec(data,size=100)\n",
    "model.save('data/wiki.model')\n",
    "print('ok')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fb27240f79425b05cfdc60ec8aab2b2dfc885c099bc04dcc326b3070e76ef9c9"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
