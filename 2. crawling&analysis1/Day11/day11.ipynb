{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b499bad6",
   "metadata": {},
   "source": [
    "#### 관련용어\n",
    "1. 크롤링<br>\n",
    "    크롤링(Crawling)이란, 웹을 탐색하는 컴퓨터 프로그램(크롤러)을 이용하여 여러 인터넷 사이트의 웹페이지\n",
    "    자료를 수집하여 분류하는 과정을 말함\n",
    "2. 스크래핑<br>\n",
    "    스크래핑(Scraping)이란, 웹 사이트의 내용을 긁어다 원하는 형태로 가공하는 기술 의미\n",
    "    즉 웹 사이트의 데이터를 수집하는 모든 작업 의미\n",
    "3. 파싱<br>\n",
    "    파싱 (parsing)이란, 어떤 페이지(문서,html 등)에서 내가 원하는 데이터를 특정 패턴이나\n",
    "    순서로 추출하여 정보를 가공하는 것을 말함. 예로 html소스를 문자열로 수집하여\n",
    "    html태그로 인식하도록 정보를 가공하여 html단위별 분석이 가능하게 구성할 수 있음.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0be8e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request     #웹에 통신 데이터를 요청하는 모듈\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "## html source 가져오기(www.naver.com)\n",
    "url='http://www.naver.com'\n",
    "resp=urllib.request.urlopen(url)\n",
    "data=resp.read()\n",
    "print(type(data))\n",
    "## html 파싱\n",
    "html=data.decode(\"utf-8\")  # byte\n",
    "type(html)  #str\n",
    "soup=BeautifulSoup(html,\"html.parser\")  #html 문서를 html소스로 파싱\n",
    "type(soup)\n",
    "## 태그 내용 가져오기\n",
    "#1.h1태그 가져오기\n",
    "h1=soup.find('h1')\n",
    "print(h1)\n",
    "print('-'*40)\n",
    "print(h1.a)\n",
    "print('-'*40)\n",
    "print(h1.a.string)\n",
    "\n",
    "# 2. find_all()\n",
    "h2=soup.find_all('h2')\n",
    "print(\"h2태그 문자열들 출력\")\n",
    "for h2_1 in h2:\n",
    "    print(h2_1.string)\n",
    "type(h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44401a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "###문제 www.naver.com에서 a태그 정보를 수집하고, 링크 문자열을 출력하는 코드를 작성\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "url=\"http://www.naver.com\"\n",
    "resp=urllib.request.urlopen(url)\n",
    "data=resp.read()\n",
    "html=data.decode('utf8')\n",
    "soup=BeautifulSoup(html,\"html.parser\")\n",
    "\n",
    "atag=soup.find_all('a')\n",
    "for a in atag:\n",
    "    print(a.string,a.attrs['href'])  #a태그 href만\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc82c135",
   "metadata": {},
   "outputs": [],
   "source": [
    "##파일을 이용하여 html01.html을 읽어 보세요\n",
    "\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "## html01.html파일읽기\n",
    "file=open('data/html01.html','r',encoding=\"utf8\")\n",
    "html01=file.read()\n",
    "html01\n",
    "#html파싱\n",
    "source = BeautifulSoup(html01,'html.parser')\n",
    "source\n",
    "\n",
    "## h1접근\n",
    "h1=source.html.body.h1      #DOM\n",
    "print(h1.string)\n",
    "\n",
    "#h2접근 : find()\n",
    "h2=source.find('h2')\n",
    "print(h2.string)\n",
    "#body 접근 children\n",
    "body=source.html.body\n",
    "for i in body.children:\n",
    "    print(i)\n",
    "\n",
    "##li 찾기\n",
    "li=source.find_all('li')\n",
    "for lis in li:\n",
    "    print(lis.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e61782",
   "metadata": {},
   "outputs": [],
   "source": [
    "##태그 속성 찾기\n",
    "#파일 읽기\n",
    "rfile2=open(\"data/html02.html\",'r',encoding='utf8')\n",
    "src2=rfile2.read()\n",
    "#html파싱\n",
    "html02=BeautifulSoup(src2,'html.parser')\n",
    "\n",
    "#a tag 찾기\n",
    "links=html02.find_all('a')\n",
    "print(\"links size=\",len(links))\n",
    "\n",
    "## a tag에서 속성 찾기\n",
    "for link in links:\n",
    "    try:\n",
    "        print(link)\n",
    "        print(link.attrs['href'])\n",
    "        print(link.attrs['target'])\n",
    "    except Exception as e:\n",
    "        print(\"예외발생 : \",e)\n",
    "\n",
    "#정규표현식으로 속성 찾기\n",
    "import re\n",
    "print(\"패턴 객체를 생성하여 속성 찾기\")\n",
    "patt=re.compile('http://')  #patt 객체...\n",
    "links=html02.find_all(href=patt)   #패턴 찾기\n",
    "links2=html02.find_all(target='_blank')\n",
    "print(links2)\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6e3e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#html03.html에서 선택자를 이용한 정보수집\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#로컬 파일 읽기\n",
    "rfile3=open('data/html03.html','r',encoding='utf8')\n",
    "src3=rfile3.read()\n",
    "\n",
    "#html파싱\n",
    "html3=BeautifulSoup(src3,'html.parser')\n",
    "\n",
    "#선택자를 이용한 내용 가져오기\n",
    "#선택자를 이용한 내용 가져오기(select, select_one)\n",
    "# 1. id='tab' 선택자 정보 가져오기\n",
    "#print(\">>>table 선택자\")\n",
    "#table=html3.select_one('#tab')  #table에 있는 id값 'tab'으로 접근\n",
    "#print(table)    #table 전체 출력 \n",
    "# 2. id선택자와 계층\n",
    "'''\n",
    "print('>>>선택자 & 계층 <<<')\n",
    "ths=html3.select('#tab>tr>th')\n",
    "print(ths)\n",
    "'''\n",
    "# 3. class선택자\n",
    "'''\n",
    "print(\">>>클래스 선택자<<<\")\n",
    "trs=html3.select('#tab>.odd')\n",
    "print(trs)\n",
    "print(\">>속성값을 이용한 선택<<\")\n",
    "trs2=html3.select('tr[class=\"odd\"]')\n",
    "print(trs2)\n",
    "'''\n",
    "##tr>td에 있는 문자열을 출력하는 코드를 작성하세요.\n",
    "print(\">>quiz<<\")\n",
    "quiz=html3.select('tr>td')\n",
    "for i in quiz:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdf309ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "카페\n",
      "블로그\n",
      "지식iN\n",
      "쇼핑\n",
      "쇼핑LIVE\n",
      "Pay\n",
      "None\n",
      "사전\n",
      "뉴스\n",
      "증권\n",
      "부동산\n",
      "지도\n",
      "VIBE\n",
      "책\n",
      "웹툰\n",
      "None\n",
      "None\n",
      "종합/경제\n",
      "방송/통신\n",
      "IT\n",
      "영자지\n",
      "스포츠/연예\n",
      "매거진/전문지\n",
      "지역\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "회사소개\n",
      "인재채용\n",
      "제휴제안\n",
      "이용약관\n",
      "개인정보처리방침\n",
      "청소년보호정책\n",
      "네이버 정책\n",
      "고객센터\n"
     ]
    }
   ],
   "source": [
    "# [실습] :  www.naver.com,www.daum.net, www.yahoo.com에서 \n",
    "# li 태그 목록정보 불러와 출력해보세요\n",
    "# [추가]=>new중 하나를 선택해서 주요 기사 내용 출력\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "url1='http://www.naver.com'\n",
    "url2='http://www.daum.net'\n",
    "url3='http://www.yahoo.com'\n",
    "resp=urllib.request.urlopen(url1)\n",
    "data=resp.read()\n",
    "html=data.decode('utf8')\n",
    "soup=BeautifulSoup(html,'html.parser')\n",
    "naver=soup.find_all('li')\n",
    "\n",
    "for na in naver:\n",
    "    print(na.string)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
